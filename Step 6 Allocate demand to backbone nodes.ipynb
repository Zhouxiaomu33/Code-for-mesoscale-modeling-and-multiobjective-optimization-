{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential packages\n",
    "import wntr\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhouxiaomu\\anaconda3\\lib\\site-packages\\wntr\\epanet\\io.py:2085: UserWarning: Not all curves were used in \"CoH_Backbone_Final_O.inp\"; added with type None, units conversion left to user\n",
      "  warnings.warn('Not all curves were used in \"{}\"; added with type None, units conversion left to user'.format(self.wn.name))\n"
     ]
    }
   ],
   "source": [
    "BWdn = wntr.network.WaterNetworkModel(r'CoH_Backbone_Final_O.inp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhouxiaomu\\anaconda3\\lib\\site-packages\\wntr\\epanet\\io.py:2085: UserWarning: Not all curves were used in \"CoH_460MGD_2018.inp\"; added with type None, units conversion left to user\n",
      "  warnings.warn('Not all curves were used in \"{}\"; added with type None, units conversion left to user'.format(self.wn.name))\n"
     ]
    }
   ],
   "source": [
    "Wdn = wntr.network.WaterNetworkModel(r'CoH_460MGD_2018.inp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate for a stable period\n",
    "Wdn.options.time.duration = 24 * 3600\n",
    "Sim = wntr.sim.EpanetSimulator(Wdn)\n",
    "Results = Sim.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the important indicators\n",
    "Flow = Results.link['flowrate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hourly hydraulic-informed directed graph \n",
    "def hourly_dgraph(wdn, flow, t): \n",
    "    # Initiated the directed graph\n",
    "    dgraph = nx.DiGraph()\n",
    "    # Use link (pipe, valve) diameter as the main selection criteria\n",
    "    for name, link in wdn.links():\n",
    "        # Determine the direction of edge through the flow rate, positive: as stored, negative: change direction\n",
    "        linkflow = flow.loc[3600*t, name]\n",
    "        # Only establish graph with nonzero flows\n",
    "        if linkflow > 1e-8:\n",
    "            start_node = link.start_node_name\n",
    "            end_node = link.end_node_name\n",
    "        elif linkflow < (-1e-8):\n",
    "            start_node = link.end_node_name\n",
    "            end_node = link.start_node_name\n",
    "        edgelist = list(dgraph.edges)\n",
    "        # Build corresponding original network\n",
    "        # If there is already a link from start_node to end_node in the graph, then this is the parallel links (pumps)\n",
    "        if (start_node, end_node) in edgelist:\n",
    "            # Keep the existing flow \n",
    "            linkcapacity = dgraph[start_node][end_node]['capacity']\n",
    "            dgraph.add_edge(start_node, end_node, linkid = name, capacity= linkcapacity + abs(linkflow))\n",
    "        else:\n",
    "            dgraph.add_node(start_node, pos = wdn.get_node(start_node).coordinates)\n",
    "            dgraph.add_node(end_node, pos = wdn.get_node(end_node).coordinates)\n",
    "            dgraph.add_edge(start_node, end_node, linkid = name, capacity= abs(linkflow))    \n",
    "    return dgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 7\n",
    "Dgraph = hourly_dgraph(Wdn, Flow, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_nodes = Wdn.node_name_list\n",
    "Backbone_nodes = BWdn.node_name_list\n",
    "Removed_nodes = [Node for Node in All_nodes if Node not in Backbone_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the locally supplied nodes and calculate fraction of their source of supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_sources = Wdn.reservoir_name_list\n",
    "Global_sources = BWdn.reservoir_name_list\n",
    "Local_sources = [r for r in All_sources if r not in Global_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supply(sources, wdn, flow, t):\n",
    "    source_link = {}\n",
    "    supply = 0\n",
    "    for name, link in wdn.links():\n",
    "        s = link.start_node_name\n",
    "        if s in sources:\n",
    "            supply += flow.loc[t*3600, name]    \n",
    "    return supply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8397102607542"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Local_supply = get_supply(Local_sources, Wdn, Flow, T)\n",
    "Local_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.83695864213047"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_supply = get_supply(All_sources, Wdn, Flow, T)\n",
    "All_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tank_supply(tank_list, wdn, flow, t):\n",
    "    tank_supply = 0\n",
    "    for name, link in wdn.links():\n",
    "        s = link.start_node_name\n",
    "        e = link.end_node_name\n",
    "        if s in tank_list:\n",
    "            # Calculate the flow out\n",
    "            tank_supply += flow.loc[t*3600, name]\n",
    "        if e in tank_list:\n",
    "            # Calculate the flow in\n",
    "            tank_supply -= flow.loc[t*3600, name]\n",
    "    return tank_supply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.902531898043444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTank_list = BWdn.tank_name_list\n",
    "Tank_list = Wdn.tank_name_list\n",
    "Tank_supply = get_tank_supply(Tank_list, Wdn, Flow, T)\n",
    "Tank_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5811331590975435"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTank_supply = get_tank_supply(BTank_list, Wdn, Flow, T)\n",
    "BTank_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.321398738945901"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Removed_tanks = [t for t in Tank_list if t not in BTank_list]\n",
    "RTank_supply = get_tank_supply(Removed_tanks, Wdn, Flow, T)\n",
    "RTank_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.573999999999998\n"
     ]
    }
   ],
   "source": [
    "print(25.735-1.321-2.840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.997248381376266"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_supply = get_supply(Global_sources, Wdn, Flow, T)\n",
    "Global_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand = Results.node['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Junction_list = Wdn.junction_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Junction_demand = {}\n",
    "for j in Junction_list:\n",
    "    Junction_demand[j] = Demand.loc[3600*T, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.735289887632234\n"
     ]
    }
   ],
   "source": [
    "print(sum(Junction_demand.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Removed_tanks = [t for t in Tank_list if t not in BTank_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cycles = list(nx.simple_cycles(Dgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cyclenodes = list(set([node for c in Cycles for node in c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(removed_nodes, backbone_nodes, cycle_downnodes, junction_demand, dgraph):\n",
    "    # If there is still demand to aggegrate\n",
    "    node_demand = {}\n",
    "    # In case some nodes are not junctions \n",
    "    node_list = list(dgraph.nodes)\n",
    "    junction_list = list(junction_demand.keys())\n",
    "    for node in node_list:\n",
    "        if node in junction_list:\n",
    "            node_demand[node] = junction_demand[node]\n",
    "        else:\n",
    "            node_demand[node] = 0    \n",
    "    dnetwork = copy.deepcopy(dgraph)\n",
    "    # Only aggregate junctions with demands\n",
    "    downstreams = [rn for rn in removed_nodes if (rn in junction_list and rn in node_list)]\n",
    "    # Initialize the downstreamspre\n",
    "    downstreamspre = downstreams\n",
    "    downstreamspost = []\n",
    "    # while we can perform aggregating and removing\n",
    "    \n",
    "    # First deal with all the simple nodes\n",
    "    while (len(downstreamspre)-len(downstreamspost)) > 0:\n",
    "        downstreamspre = copy.deepcopy(downstreams)\n",
    "        downstreams_updated = copy.deepcopy(downstreams) \n",
    "        for node in downstreams:\n",
    "            if node not in cycle_downnodes:\n",
    "                # Only aggegrate nodes that have no sucessors\n",
    "                nodesucc = list(dnetwork.successors(node))\n",
    "                if len(nodesucc) == 0:\n",
    "                    # Aggeragated its deamnd to its immediate upstream nodes\n",
    "                    nodeprede = list(dnetwork.predecessors(node))\n",
    "                    q = []\n",
    "                    for k in range(len(nodeprede)):\n",
    "                        # Get the flow from k to j, which is the capacity established before\n",
    "                        q.append(dnetwork[nodeprede[k]][node]['capacity'])\n",
    "                    q = np.array(q)\n",
    "                    sumq = sum(q)\n",
    "                    for k in range(len(nodeprede)):\n",
    "                        node_demand[nodeprede[k]] += (q[k]/sumq)*node_demand[node]\n",
    "                    downstreams_updated.remove(node)\n",
    "                    dnetwork.remove_node(node)\n",
    "        downstreams = copy.deepcopy(downstreams_updated)\n",
    "        downstreamspost = copy.deepcopy(downstreams_updated)\n",
    "    # Then deal with nodes in the cycles\n",
    "    downstreamspost = []\n",
    "    while (len(downstreamspre)-len(downstreamspost)) > 0:\n",
    "        downstreamspre = copy.deepcopy(downstreams)\n",
    "        downstreams_updated = copy.deepcopy(downstreams) \n",
    "        for node in downstreams:\n",
    "            if node in cycle_downnodes:\n",
    "                nodesucc = list(dnetwork.successors(node))\n",
    "                upstreams = list(dnetwork.predecessors(node))\n",
    "                # Start from the simple nodes to break the cycle\n",
    "                if (len(nodesucc) == 1 and len(upstreams) == 1):\n",
    "                    # Aggeragated its deamnd to its immediate upstream and downstream nodes\n",
    "                    node_demand[nodesucc[0]] += node_demand[node]/2\n",
    "                    node_demand[upstreams[0]] += node_demand[node]/2\n",
    "                    downstreams_updated.remove(node)\n",
    "                    dnetwork.remove_node(node)\n",
    "                    cycle_downnodes.remove(node)\n",
    "                if (len(nodesucc) == 1 and len(upstreams) == 0):\n",
    "                    node_demand[nodesucc[0]] += node_demand[node]\n",
    "                    downstreams_updated.remove(node)\n",
    "                    dnetwork.remove_node(node)\n",
    "                    cycle_downnodes.remove(node)\n",
    "                if len(nodesucc) == 0:\n",
    "                    nodeprede = list(dnetwork.predecessors(node))\n",
    "                    q = []\n",
    "                    for k in range(len(nodeprede)):\n",
    "                        # Get the flow from k to j, which is the capacity established before\n",
    "                        q.append(dnetwork[nodeprede[k]][node]['capacity'])\n",
    "                    q = np.array(q)\n",
    "                    sumq = sum(q)\n",
    "                    for k in range(len(nodeprede)):\n",
    "                        node_demand[nodeprede[k]] += (q[k]/sumq)*node_demand[node]\n",
    "                    downstreams_updated.remove(node)\n",
    "                    dnetwork.remove_node(node)\n",
    "                    cycle_downnodes.remove(node)\n",
    "        downstreams = copy.deepcopy(downstreams_updated)\n",
    "        downstreamspost = copy.deepcopy(downstreams_updated)\n",
    "    # Deal with the leftover nodes\n",
    "    ldownstreamspre = copy.deepcopy(downstreams)\n",
    "    ldownstreamspost = []\n",
    "    while (len(ldownstreamspre)-len(ldownstreamspost)) > 0:\n",
    "        ldownstreamspre = copy.deepcopy(downstreams)\n",
    "        ldownstreams_updated = copy.deepcopy(downstreams)\n",
    "        for node in downstreams:\n",
    "            nodesucc = list(dnetwork.successors(node))\n",
    "            # For the node that is both upstream and downstream of backbone nodes\n",
    "            if (set(nodesucc).issubset(set(backbone_nodes)) or len(nodesucc) == 0):\n",
    "                nodeprede = list(dnetwork.predecessors(node))\n",
    "                q = []\n",
    "                for k in range(len(nodeprede)):\n",
    "                    # Get the flow from k to j, which is the capacity established before\n",
    "                    q.append(dnetwork[nodeprede[k]][node]['capacity'])\n",
    "                q = np.array(q)\n",
    "                sumq = sum(q)\n",
    "                for k in range(len(nodeprede)):\n",
    "                    node_demand[nodeprede[k]] += (q[k]/sumq)*node_demand[node]\n",
    "                ldownstreams_updated.remove(node)\n",
    "                dnetwork.remove_node(node)\n",
    "        downstreams = copy.deepcopy(ldownstreams_updated)\n",
    "        ldownstreamspost = copy.deepcopy(ldownstreams_updated)\n",
    "    backbone_demand = {}\n",
    "    print(downstreams)\n",
    "    for bn in backbone_nodes:\n",
    "        backbone_demand[bn] = node_demand[bn]\n",
    "    return backbone_demand, dnetwork, node_demand              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['129', '130', '233', '242', '243', '365', '366', '367', '368', '375', '376', '379', '380', '381', '382', '383', '384', '385', '386', '390', '391', '424', '425', '434', '435', '436', '437', '438', '439', '440', '441', '443', '446', '447', '450', '451', '452', '453', '456', '457', '466', '467', '470', '471', '472', '473', '476', '477', '482', '483', '8092', '8152', '8138', '8126', '8120', '8158', '8172', '8164', '8166', '8144', '8128', '8142', '8132', '8086', '8090']\n"
     ]
    }
   ],
   "source": [
    "Backbone_demand, Dnetwork, Node_demand = aggregate(Removed_nodes, Backbone_nodes, Cyclenodes, Junction_demand, Dgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.588478115112597\n"
     ]
    }
   ],
   "source": [
    "print(sum(Backbone_demand.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Withdraw the backbone node that is not junction but assigned with demand\n",
    "for key, value in Backbone_demand.items():\n",
    "    if (key not in Junction_list) and value > 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the non-junctions\n",
    "Non_junctions = Global_sources + BTank_list\n",
    "for key in Non_junctions:\n",
    "    if key in Backbone_demand.keys():\n",
    "        del Backbone_demand[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to inp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_junctions(filename, bwdn, demand_hourly):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for key, value in demand_hourly.items():\n",
    "            junctionobject = bwdn.get_node(key)\n",
    "            file.write(key)\n",
    "            # To keep the string left aligned\n",
    "            spacelength = (20 - len(key))\n",
    "            for k in range(spacelength):\n",
    "                file.write(' ')\n",
    "            # Write the elevation value We need to transfer the unit\n",
    "            file.write(str(np.around((junctionobject.elevation * 3.28084), decimals=2)))\n",
    "            spacelength = (20 - len(str(np.around((junctionobject.elevation * 3.28084), decimals=2))))\n",
    "            for k in range(spacelength):\n",
    "                file.write(' ')\n",
    "            # Write the basline value, transfer to gpm\n",
    "            file.write(str(np.around((demand_hourly[key] * 15850.3*1.61), decimals=2)))\n",
    "            spacelength = (20 - len(str(np.around((demand_hourly[key] * 15850.3*1.61), decimals=2))))\n",
    "            for k in range(spacelength):\n",
    "                file.write(' ')\n",
    "            # Write the patterns\n",
    "            file.write( 'Pat0' + ';\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_junctions('Aggregate_O_demand_7am.inp', BWdn, Backbone_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backbone_down_remove(backbone_nodes,removed_nodes, dgraph):\n",
    "    backbone_remove = {}\n",
    "    for i in range(len(backbone_nodes)):\n",
    "        upstreams = []\n",
    "        alldownstreams = []\n",
    "        upstreams.append(backbone_nodes[i])\n",
    "        for node in upstreams:\n",
    "            try:\n",
    "                node_successors = list(dgraph.successors(node))\n",
    "                for d in node_successors:\n",
    "                    # If there is still downstream neigbors in the removed_nodes\n",
    "                    if d in removed_nodes:\n",
    "                        # To make sure that the nodes in cycles will not to be visisted twice\n",
    "                        if d not in upstreams:\n",
    "                            upstreams.append(d)\n",
    "                            alldownstreams.append(d)\n",
    "            except:\n",
    "                pass\n",
    "        backbone_remove[backbone_nodes[i]] = alldownstreams\n",
    "    return backbone_remove         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Backbone_remove = backbone_down_remove(Backbone_nodes, Removed_nodes, Dgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_back(backbone_remove, removed_nodes):\n",
    "    remove_backbone = {}\n",
    "    for node in removed_nodes:\n",
    "        remove_backbone[node] = []\n",
    "    for key, value in backbone_remove.items():\n",
    "        if len(value) > 0:\n",
    "            for i in value:\n",
    "                remove_backbone[i].append(key)\n",
    "    return remove_backbone       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove_backbone = map_back(Backbone_remove, Removed_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the graph that only include the backbone nodes and the removed_nodes and removed_links\n",
    "def partial_graph(backbone_network, dgraph):\n",
    "    whoeldgraph = copy.deepcopy(dgraph)\n",
    "    # Get the edges in backbone_network\n",
    "    backbone_edges = list(backbone_network.edges())\n",
    "    whole_edges = list(dgraph.edges())\n",
    "    for edge in backbone_edges:\n",
    "        (sn, en) = edge\n",
    "        linkid = backbone_network[sn][en]['linkid']\n",
    "        for edgew in whole_edges:\n",
    "            (snw, enw) = edgew\n",
    "            if linkid == dgraph[snw][enw]['linkid']:\n",
    "                whoeldgraph.remove_edge(snw, enw)\n",
    "    return whoeldgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the backbone network\n",
    "FBackbone_network = open(r'CoH_Backbone_network.pickle', 'rb')\n",
    "Backbone_network = pickle.load(FBackbone_network)\n",
    "FBackbone_network.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pgraph = partial_graph(Backbone_network, Dgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrackalgorithm(backbone_remove, dgraph):\n",
    "    r = {}\n",
    "    maximumdistance = {}\n",
    "    # For each backbone node find its downstream removed nodes \n",
    "    for i, jlist in backbone_remove.items():\n",
    "        r[i] = {}\n",
    "        maximumdistance[i] = {}\n",
    "        # Intialize the and rij\n",
    "        r[i][i] = 1\n",
    "        allnodes = jlist + [i] \n",
    "        # If the node has downstream removed nodes\n",
    "        if len(jlist) > 0:\n",
    "            for j in jlist:\n",
    "                # j is accessible from i \n",
    "                paths = list(nx.all_simple_paths(dgraph, source=i, target=j))\n",
    "                maximumd = 0\n",
    "                for p in paths:\n",
    "                    d = len(p)\n",
    "                    if d > maximumd:\n",
    "                        maximumd = d\n",
    "                maximumdistance[i][j] = maximumd\n",
    "        # Rank the nodes according to their maximum distance to the source\n",
    "        nodetoevaluate = sorted(maximumdistance[i], key=maximumdistance[i].get)\n",
    "        for j in nodetoevaluate:\n",
    "            r[i][j] = 0\n",
    "            # Get its upstream nodes that belong to the nodes to consider forward tracking\n",
    "            klist = list(dgraph.predecessors(j))\n",
    "            # As for the nodes that are not accessible from i, initialize its r[i][k] = 0\n",
    "            for k in klist:\n",
    "                if k not in allnodes:\n",
    "                    r[i][k] = 0\n",
    "            qkj = []\n",
    "            for ik in range(len(klist)):\n",
    "                # Get the flow from k to j, which is the capacity established before\n",
    "                qkj.append(dgraph[klist[ik]][j]['capacity'])\n",
    "            qkj = np.array(qkj)\n",
    "            rkj = qkj/sum(qkj)\n",
    "            for ik in range(len(klist)):\n",
    "                r[i][j] += r[i][klist[ik]]*rkj[ik]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = backtrackalgorithm(Backbone_remove, Pgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_demand(backbone_nodes, remove_backbone, junction_demand):\n",
    "    node_allocation = {}\n",
    "    junction_list = list(junction_demand.keys())\n",
    "    for node in backbone_nodes:\n",
    "        if node in junction_list:\n",
    "            # Initialize the allocation as its original value\n",
    "            node_allocation[node] = junction_demand[node]\n",
    "        else:\n",
    "            node_allocation[node] = 0\n",
    "    for remove, value in remove_backbone.items():\n",
    "        if len(value) > 0:\n",
    "            if remove in junction_list:\n",
    "                remove_demand = junction_demand[remove]\n",
    "            else:\n",
    "                remove_demand = 0\n",
    "            for i in value:\n",
    "                # Allocate the demand averagily to its upstream nodes\n",
    "                node_allocation[i] += remove_demand/len(value)\n",
    "    return node_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.258907076022005\n"
     ]
    }
   ],
   "source": [
    "print(sum(Junction_demand.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node_allocation = allocate_demand(Backbone_nodes, Remove_backbone, Junction_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.428286327073426\n"
     ]
    }
   ],
   "source": [
    "print(sum(Node_allocation.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the reservoir nodes\n",
    "for key in Global_sources:\n",
    "    del Node_allocation[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the tank nodes\n",
    "for key in BTank_list:\n",
    "    del Node_allocation[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7937849693291585"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20.428286327073426/25.735289929132477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrackalgorithm(backbone_remove, dgraph):\n",
    "    r = {}\n",
    "    # For each backbone node find its downstream removed nodes \n",
    "    for i, jlist in backbone_remove.items():\n",
    "        r[i] = {}\n",
    "        print(i, jlist)\n",
    "        # Intialize the and rij\n",
    "        r[i][i] = 1\n",
    "        # If the node has downstream removed nodes\n",
    "        if len(jlist) > 0:\n",
    "            # All the nodes to consider for our forward tracking \n",
    "            allnodes = [i] + jlist\n",
    "            jlength = len(jlist)\n",
    "            # print(jlist)\n",
    "            # Range the j list according to the downstream level\n",
    "            for j in jlist:\n",
    "                j_predecessors = list(dgraph.predecessors(j))\n",
    "                for jp in j_predecessors:\n",
    "                    # If its updtream node is also in the jlist, check its index\n",
    "                    if jp in jlist:\n",
    "                        jpindex, jindex = jlist.index(jp), jlist.index(j)\n",
    "                        # If its upstream is after it \n",
    "                        if jpindex > jindex:\n",
    "                            # Inset this element behind its upstream node\n",
    "                            jlist.insert((jpindex+1), j)\n",
    "            # Need to withdraw the last jlength of elements in jlist\n",
    "            jlist_updated = jlist[(-jlength):]\n",
    "            for j in jlist_updated:\n",
    "                r[i][j] = 0\n",
    "                # Get its upstream nodes that belong to the nodes to consider forward tracking \n",
    "                allpredecessors = list(dgraph.predecessors(j))\n",
    "                klist = [node for node in allpredecessors if node in allnodes]\n",
    "                qj = 0\n",
    "                qkj = []\n",
    "                for ik in range(len(klist)):\n",
    "                    # Get the flow from k to j, which is the capacity established before\n",
    "                    qkj.append(dgraph[klist[ik]][j]['capacity'])\n",
    "                qkj = np.array(qkj)\n",
    "                rkj = qkj/sum(qkj)\n",
    "                for ik in range(len(klist)):\n",
    "                    r[i][j] += r[i][klist[ik]]*rkj[ik]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the nodes that have combined supply\n",
    "def get_combined_junctions(local_sources, global_sources, dgraph):\n",
    "    local_junctions = []\n",
    "    for ls in local_sources:\n",
    "        try:\n",
    "            ls_des = nx.descendants(dgraph, ls)\n",
    "            local_junctions += ls_des\n",
    "        except:\n",
    "            pass\n",
    "    global_junctions = []\n",
    "    for gs in global_sources:\n",
    "        try:\n",
    "            gs_des = nx.descendants(dgraph, gs)\n",
    "            global_junctions += gs_des\n",
    "        except:\n",
    "            pass\n",
    "    local_junctions = set(local_junctions)\n",
    "    global_junctions = set(global_junctions)\n",
    "    combined_junctions = list(local_junctions.intersection(global_junctions))\n",
    "    return combined_junctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tseries = np.linspace(0, 23, 24)\n",
    "S1 = 0\n",
    "S2 = 0\n",
    "D = 0\n",
    "for t in Tseries:\n",
    "    S1 += get_local_supply(All_sources, Wdn, Flow, t)\n",
    "    S2 += get_tank_supply(Tank_list, Wdn, Flow, t)\n",
    "    Junction_demand = {}\n",
    "    for j in Junction_list:\n",
    "        Junction_demand[j] = Demand.loc[3600*t, j]\n",
    "    D += (sum(Junction_demand.values())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expected = wntr.metrics.hydraulic.expected_demand(Wdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSA = wntr.metrics.hydraulic.water_service_availability(Expected, Demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.633000000000003\n"
     ]
    }
   ],
   "source": [
    "print((11.121+5.372+2.140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.364\n"
     ]
    }
   ],
   "source": [
    "print(11.402+4.804+2.158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01443675199914122\n"
     ]
    }
   ],
   "source": [
    "print((18.364-18.633)/18.633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
